{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5e1948",
   "metadata": {},
   "source": [
    "## Let's train an MLP using JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3c8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from jax import grad, jit, vmap, pmap\n",
    "\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from typing import Tuple, NamedTuple\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5d100",
   "metadata": {},
   "source": [
    "Note: zip(list[:-1],list[1:]) creates pair of elements from the list <br>\n",
    "<br>\n",
    "For example if list = [1, 128, 128, 1]<br>\n",
    "zip(list[:-1],list[1:]) = zip( [1, 128, 128], [128, 128, 1] )<br>\n",
    "Result will be (1,128), (128, 128), (128,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39687015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_params(layer_widths):\n",
    "    params = []\n",
    "\n",
    "    # Allocates weights and biases (model parameters)\n",
    "    # Notice: we are not using JAX's PRNG here - it wont matter for this example\n",
    "    for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
    "        params.append(\n",
    "            dict(weights = np.random.normal(size=(n_in, n_out))*np.sqrt(2/n_in),\n",
    "            biases = np.ones(shape=(n_out,))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce678dc3",
   "metadata": {},
   "source": [
    "Multiplying by sqrt(2/n_in) is \"He Initialization\"<br>\n",
    "It's a clever trick used to train NNs better<br>\n",
    "If initially weights too large -> Large output (Exploding Activations)<br>\n",
    "If initially weights too small -> Small output (Vanishing Activations)<br><br>\n",
    "By multiplying by that value, we aim to keep the variance stable<br>\n",
    "If there are more inputs -> The weights will be smaller<br>\n",
    "If there are less inputs -> The weights will be larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af63939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a single input - single output, 3 layer (2 hidden) deep MLP\n",
    "params = init_mlp_params([1, 128, 128, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117730a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'biases': (128,), 'weights': (1, 128)},\n",
       " {'biases': (128,), 'weights': (128, 128)},\n",
       " {'biases': (1,), 'weights': (128, 1)}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example of how we might use tree.map: To verify the shapes make sense\n",
    "jax.tree.map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e32553",
   "metadata": {},
   "source": [
    "#### The above function creates the initial state in a way that:\n",
    "- Is explicit\n",
    "- Can be passed as function arguments\n",
    "- Works with JAX Transforms\n",
    "- Can be organized as a PyTree for easy manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46454a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
