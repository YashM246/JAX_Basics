{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46e1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from jax import grad, jit, vmap, pmap\n",
    "\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from typing import Tuple, NamedTuple\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5bf39f",
   "metadata": {},
   "source": [
    "## Parallelism in JAX<br>\n",
    "#### Parallelism in JAX is handled by a function called pmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd38fd71",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "jax.tools.colab_tpu.setup_tpu() was required for older JAX versions running on older generations of TPUs, and should no longer be used.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get ourselves some TPU Goodness\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab_tpu\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolab_tpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_tpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m jax.devices()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gr8my\\Desktop\\Projects\\JAX_Basics\\env\\Lib\\site-packages\\jax\\tools\\colab_tpu.py:20\u001b[39m, in \u001b[36msetup_tpu\u001b[39m\u001b[34m(tpu_driver_version)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetup_tpu\u001b[39m(tpu_driver_version=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     19\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Raises an error. Do not use.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mjax.tools.colab_tpu.setup_tpu() was required for older JAX versions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m running on older generations of TPUs, and should no longer be used.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: jax.tools.colab_tpu.setup_tpu() was required for older JAX versions running on older generations of TPUs, and should no longer be used."
     ]
    }
   ],
   "source": [
    "# Get ourselves some TPU Goodness\n",
    "import jax.tools.colab_tpu\n",
    "jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a1fe35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devies: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "devices = jax.devices()\n",
    "print(f\"Available devies: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4cd9fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default backend: cpu\n"
     ]
    }
   ],
   "source": [
    "# If above code does not show GPU, jax is not properly configured\n",
    "\n",
    "# Check default backend\n",
    "print(f\"Default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526543fa",
   "metadata": {},
   "source": [
    "### Note: JAX does not support native GPU installations on Windows<br> \n",
    "#### It only supports GPU acceleration on Windows through the use of WSL2 (Windows Subsystem for Linux 2). \n",
    "#### For direct installation on Windows, only CPU support is officially available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3bf7f",
   "metadata": {},
   "source": [
    "#### (We can still learn JAX concepts on CPU or use Colab TPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eae33416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple running example\n",
    "x = np.arange(5)                # Signal\n",
    "w = np.array([2., 3., 4.])      # Kernel/Window\n",
    "\n",
    "# 1D Convolution\n",
    "def convolve(w, x):\n",
    "    op = []\n",
    "\n",
    "    for i in range(1, len(x)-1):\n",
    "        op.append(jnp.dot(x[i-1:i+2], w))\n",
    "\n",
    "    return jnp.array(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f24d13",
   "metadata": {},
   "source": [
    "- repr(): String Representation of Object\n",
    "- Shows what the object is (type and identity) unambiguously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59135766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([11., 20., 29.], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "result = convolve(w, x)\n",
    "print(repr(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c149a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available devices: 1\n"
     ]
    }
   ],
   "source": [
    "n_devices = jax.local_device_count()\n",
    "print(f\"Number of available devices: {n_devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d56d5e",
   "metadata": {},
   "source": [
    "Here you would see more than one (depending upon how many cores your GPU/TPU has)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb29b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5) (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Imagine we have a heavier load (a batch of examples)\n",
    "xs = np.arange(5*n_devices).reshape(-1, 5)\n",
    "ws = np.stack([w]*n_devices)\n",
    "\n",
    "print(xs.shape, ws.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06c647",
   "metadata": {},
   "source": [
    "#### First Way to Optimize: Simply use vmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfc4d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([[11., 20., 29.]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vmap_result = jax.vmap(convolve)(ws, xs)\n",
    "# By default, the in_axes argument in set to (0,0)\n",
    "print(repr(vmap_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4abf5a5",
   "metadata": {},
   "source": [
    "- vmap automatically vectorizes a function to work on batches of data, without us having to write explicit loops or manual batching code\n",
    "- in_axes tells vmap with acis to map over for each argument\n",
    "- Eg: jax.vmap(func, in_axes=(arg1_axis, arg2_axis, ...))(arg1, arg2, ...)\n",
    "- 0 = map over axis 0 (first dim, which is usually the batch dimension)\n",
    "- 1 = map over axis 1\n",
    "- None: don't map, use same value for all (broadcasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78658e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([[11., 20., 29.]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# The amazing part is, if you just swap vmap for pmap, you are now \n",
    "# running on multiple devices\n",
    "\n",
    "pmap_result = jax.pmap(convolve)(ws, xs)\n",
    "print(repr(pmap_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3d4f4",
   "metadata": {},
   "source": [
    "- All of this happens independently\n",
    "- There are no cross-device communication costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab9b71b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([[11., 20., 29.]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Same operation, but smarter way\n",
    "# We dont have to manually broadcast w\n",
    "\n",
    "sol = jax.pmap(convolve, in_axes=(None, 0))(w, xs)\n",
    "print(repr(sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487f8e6",
   "metadata": {},
   "source": [
    "#### All this is great, but sometimes we do require communication between devices!<br><br>\n",
    "Let's see how this is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "774d186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Convolution Example, but this time\n",
    "# we communicate across devices to normalize the outputs\n",
    "\n",
    "def normalized_convolve(w, x):\n",
    "    output = []\n",
    "    for i in range(1, len(x)-1):\n",
    "        output.append(jnp.dot(x[i-1:i+2], w))\n",
    "    output = jnp.array(output)\n",
    "    # Same as before till here\n",
    "\n",
    "    output = output/jax.lax.psum(output, axis_name='batch_dim')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00420211",
   "metadata": {},
   "source": [
    "jax.lax.psum() --> \"Parallel Sum\": Sums values across multiple devices (GPUs/TPUs)<br><br>\n",
    "Takes in arguments:\n",
    "- x: value to sum (array or scalar)\n",
    "- axis_name: which parallel axis to sum across"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bc300c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pmap = jax.pmap(normalized_convolve, axis_name='batch_dim', in_axes=(None, 0))(w, xs)\n",
    "res_vmap = jax.vmap(normalized_convolve, axis_name='batch_dim', in_axes=(None, 0))(w, xs)\n",
    "# axis_name just gives an arbitrary name to mapped axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e618e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([[1., 1., 1.]], dtype=float32)\n",
      "Array([[1., 1., 1.]], dtype=float32)\n",
      "Verify output is normalized: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(repr(res_pmap))\n",
    "print(repr(res_vmap))\n",
    "print(f\"Verify output is normalized: {sum(res_pmap[:,0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914b7ba",
   "metadata": {},
   "source": [
    "#### Couple more useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c476b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aside from grads, we also need to return losses\n",
    "\n",
    "def sum_squared_error(x, y):\n",
    "    return sum((x-y)**2)\n",
    "\n",
    "x = jnp.arange(4, dtype=jnp.float32)\n",
    "y = x + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a8f3240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3.]\n",
      "(Array(0.03999997, dtype=float32), Array([-0.2       , -0.20000005, -0.19999981, -0.19999981], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Efficient Way to return grad and loss value\n",
    "print(x)\n",
    "print(jax.value_and_grad(sum_squared_error)(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "794aebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, the loss function also needs to return intermediate results\n",
    "\n",
    "def sum_squared_error_with_aux(x, y):\n",
    "    return sum((x-y)**2), (x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will throw Error - Gradient only works for scalar output functions\n",
    "jax.grad(sum_squared_error_with_aux)(x,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "294853fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-0.2       , -0.20000005, -0.19999981, -0.19999981], dtype=float32),\n",
       " Array([-0.1       , -0.10000002, -0.0999999 , -0.0999999 ], dtype=float32))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "jax.grad(sum_squared_error_with_aux, has_aux=True)(x,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cf4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
